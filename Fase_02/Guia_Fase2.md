# üìä Plan Detallado - Fase 2: Exploraci√≥n y Limpieza de Datos (EDA)

## üéØ Objetivo Principal de Esta Fase

El objetivo principal de esta fase es capacitarte para que puedas entender, inspeccionar y preparar cualquier dataset para su posterior uso en modelos de Machine Learning. Aprender√°s a identificar problemas comunes en los datos y a aplicar t√©cnicas efectivas para limpiarlos y transformarlos, asegurando que tus modelos trabajen con la mejor informaci√≥n posible.

## üóùÔ∏è Analog√≠a Clave de la Fase: El Detective de Datos

Imagina que eres un "detective de datos". Tu misi√≥n es examinar cuidadosamente la "escena del crimen" (tu dataset) para encontrar pistas (patrones, relaciones), identificar "sospechosos" (valores at√≠picos, anomal√≠as), y limpiar cualquier "evidencia falsa o contaminada" (valores faltantes, duplicados) antes de poder resolver el caso (construir un modelo predictivo fiable).

## üõ†Ô∏è Herramientas Clave en Python

* **Pandas:** La librer√≠a fundamental para la manipulaci√≥n, an√°lisis y estructuraci√≥n de datos. Ser√° tu principal aliada.
* **Matplotlib y Seaborn:** Librer√≠as de visualizaci√≥n de datos esenciales para crear gr√°ficos informativos y entender las distribuciones y relaciones.
* **NumPy:** Base para operaciones num√©ricas eficientes, especialmente cuando trabajemos con arrays de datos.
* **Scikit-learn:** Aunque es conocida por el modelado, tambi√©n ofrece herramientas cruciales para el preprocesamiento de datos (imputaci√≥n, escalado, codificaci√≥n).
* **(Opcional) Missingno:** Una peque√±a librer√≠a que facilita la visualizaci√≥n de patrones de valores faltantes.

## üìä Dataset Principal para la Fase 2: Melbourne Housing Snapshot

**¬øPor qu√© este dataset?**
El dataset **Melbourne Housing Snapshot** es un conjunto de datos real y manejable que utilizaremos como ejemplo principal a lo largo de esta fase. Contiene una rica mezcla de:
* **Caracter√≠sticas Reales y Categoricas:** Permite practicar la identificaci√≥n de tipos de datos.
* **Valores Faltantes:** Presenta diversos patrones de valores ausentes en varias columnas, ideal para practicar estrategias de imputaci√≥n.
* **Outliers:** Contiene valores at√≠picos que requieren detecci√≥n y manejo.
* **Relaciones Complejas:** Permite explorar correlaciones y relaciones entre variables para la detecci√≥n de patrones.

Su complejidad moderada lo hace perfecto para aplicar todas las t√©cnicas de EDA y limpieza que aprenderemos, sin ser abrumador.

---

## üìö M√≥dulos y Contenido Detallado de la Fase 2

### M√≥dulo 2.1: Exploraci√≥n Inicial del Dataset

**Contenido:** En este m√≥dulo, aprender√°s a realizar la primera "investigaci√≥n" de un dataset: c√≥mo cargarlo, entender su estructura b√°sica, identificar tipos de datos y obtener una visi√≥n general de sus caracter√≠sticas.

#### 1. Primer Contacto con el Dataset: Carga e Inspecci√≥n Inicial

* **C√≥mo cargar un dataset en Python:** `pd.read_csv()`.
* **Inspecci√≥n R√°pida:**
    * `df.head()`, `df.tail()`, `df.sample()`: Para ver las primeras/√∫ltimas/muestras de filas.
    * `df.shape`: Dimensi√≥n (filas, columnas).
    * `df.info()`: Resumen de tipos de datos y conteo de valores no nulos.
    * `df.describe()`: Estad√≠sticas descriptivas b√°sicas para columnas num√©ricas.
    * `df.columns`: Nombres de las columnas.
    * **Identificaci√≥n de Valores √önicos:** `.nunique()` para cada columna.
    * **An√°lisis de Valores Frecuentes:** `.value_counts()` para variables categ√≥ricas y discretas.

#### 2. Visualizaci√≥n de Datos para la Exploraci√≥n

* **Histogramas:** Para visualizar la distribuci√≥n de variables num√©ricas. Aprender√°s a interpretar la forma, el sesgo y la presencia de m√∫ltiples modos.
* **Diagramas de Caja (Boxplots):** Esenciales para visualizar la distribuci√≥n de variables num√©ricas, identificar la mediana, cuartiles y, crucialmente, la presencia de posibles outliers.
* **Diagramas de Dispersi√≥n (Scatter Plots):** Para explorar la relaci√≥n entre dos variables num√©ricas, buscando correlaciones o patrones.
* **Gr√°ficos de Barras (`countplot`):** Para visualizar la frecuencia de las categor√≠as en variables categ√≥ricas.
* **Mapas de Calor (`heatmap`):** Para visualizar la matriz de correlaci√≥n entre m√∫ltiples variables num√©ricas, identificando r√°pidamente relaciones fuertes.
* **Diagramas de Pares (`pairplot`):** Una herramienta poderosa para visualizar relaciones entre m√∫ltiples pares de variables de una vez.
* **Visualizaciones Num√©ricas vs. Categ√≥ricas:** Boxplots o violin plots para comparar la distribuci√≥n de una variable num√©rica a trav√©s de las categor√≠as de otra.

#### 3. Ejercicios Complementarios (M√≥dulo 2.1)

* **Ejercicio 1:** Carga el dataset "Melbourne Housing Snapshot" y realiza una inspecci√≥n inicial completa: `head()`, `info()`, `describe()`, `shape`.
* **Ejercicio 2:** Identifica y muestra los 5 valores m√°s frecuentes de al menos 3 columnas categ√≥ricas (`value_counts()`).
* **Ejercicio 3:** Genera histogramas para 3 variables num√©ricas diferentes y boxplots para las mismas 3 variables. Interpreta sus distribuciones.
* **Ejercicio 4:** Crea un diagrama de dispersi√≥n entre '√Årea de la Tierra' y 'Precio' en el dataset. ¬øObservas alguna relaci√≥n?
* **Ejercicio 5:** Calcula la matriz de correlaci√≥n de las variables num√©ricas y visual√≠zala con un `heatmap`. ¬øQu√© correlaciones te parecen m√°s interesantes?
* **Ejercicio 6:** Utiliza un `boxplot` para visualizar la distribuci√≥n de 'Precio' seg√∫n el 'Tipo de Propiedad'.

### M√≥dulo 2.2: Manejo de Valores Faltantes y Duplicados

**Contenido:** Una vez que hemos explorado los datos, es com√∫n encontrar "agujeros" (valores faltantes) o entradas repetidas (duplicados). Este m√≥dulo te dar√° las herramientas para gestionarlos.

#### 1. Detecci√≥n y Tratamiento de Valores Faltantes

* **Identificaci√≥n:**
    * `df.isnull()`, `df.isna()`: Para identificar valores nulos.
    * `df.isnull().sum()`: Conteo de valores faltantes por columna.
    * `df.isnull().sum() / len(df) * 100`: Porcentaje de valores faltantes por columna.
    * **Visualizaci√≥n de patrones de faltantes (ej. librer√≠a `missingno`)**.
* **Estrategias de Eliminaci√≥n:**
    * `df.dropna(axis=0)`: Eliminaci√≥n de filas con cualquier valor faltante.
    * `df.dropna(axis=1)`: Eliminaci√≥n de columnas con cualquier valor faltante.
    * Discusi√≥n: ¬øCu√°ndo eliminar filas o columnas? (Umbrales de % de faltantes).
* **Estrategias de Imputaci√≥n (Relleno):**
    * **Imputaci√≥n con un valor constante:** `df.fillna(valor)`.
    * **Imputaci√≥n con la Media:** `.fillna(df['col'].mean())` (para variables num√©ricas).
    * **Imputaci√≥n con la Mediana:** `.fillna(df['col'].median())` (m√°s robusta a outliers).
    * **Imputaci√≥n con la Moda:** `.fillna(df['col'].mode()[0])` (para variables categ√≥ricas o discretas).
    * **Introducci√≥n a `SimpleImputer` de Scikit-learn:** Uso de diferentes estrategias ('mean', 'median', 'most_frequent', 'constant').
    * Consideraciones sobre la imputaci√≥n avanzada (ej. por regresi√≥n, K-NN Imputer, menci√≥n).

#### 2. Manejo de Valores Duplicados

* **Detecci√≥n de Duplicados:**
    * `df.duplicated()`: Identifica filas completas que son duplicados.
    * `df.duplicated(subset=['col1', 'col2'])`: Identifica duplicados basados en un subconjunto de columnas.
    * `df.sum()` para contar duplicados.
* **Eliminaci√≥n de Duplicados:**
    * `df.drop_duplicates()`: Elimina filas duplicadas.
    * Discusi√≥n: ¬øCu√°ndo eliminar y qu√© duplicado conservar (primer/√∫ltimo)?

#### 3. Ejercicios Complementarios (M√≥dulo 2.2)

* **Ejercicio 1:** Carga el dataset de Melbourne. Cuantifica los valores faltantes por columna y muestra el porcentaje. Visual√≠zalos usando `missingno.matrix()`.
* **Ejercicio 2:** Elige una columna num√©rica con faltantes y rell√©nala con la mediana. Elige otra y rell√©nala con un valor constante que tenga sentido.
* **Ejercicio 3:** Elige una columna categ√≥rica con faltantes y rell√©nala con la moda.
* **Ejercicio 4:** Identifica y cuenta cu√°ntas filas duplicadas exactas hay en el dataset. Elimina esas filas duplicadas.
* **Ejercicio 5:** Investiga si hay duplicados basados solo en las columnas 'Direcci√≥n' y 'Fecha de Venta'. Si los hay, elim√≠nalos conservando la primera aparici√≥n.

### M√≥dulo 2.3: Detecci√≥n y Manejo de Outliers y Anomal√≠as

**Contenido:** Este m√≥dulo explora c√≥mo identificar "puntos extra√±os" en tus datos que podr√≠an afectar el rendimiento de tus modelos.

#### 1. Definici√≥n y Diferencia: Outliers vs. Anomal√≠as

* **Outliers:** Puntos de datos que se desv√≠an significativamente de la mayor√≠a de los otros puntos.
* **Anomal√≠as:** Eventos raros o inusuales que no siguen el patr√≥n esperado del resto de los datos (puede ser m√°s amplio que un simple outlier).
* ¬øPor qu√© es importante detectarlos? Impacto en medias, varianzas y modelos.

#### 2. M√©todos de Detecci√≥n de Outliers

* **Visualizaci√≥n:**
    * **Diagramas de Caja (Boxplots):** La herramienta visual m√°s com√∫n. Explicaci√≥n de los "bigotes" y los puntos fuera de ellos.
    * Histogramas y gr√°ficos de densidad: Para identificar colas largas o picos inusuales.
    * Diagramas de dispersi√≥n: Para identificar puntos at√≠picos en relaciones entre dos variables.
* **M√©todos Estad√≠sticos:**
    * **Rango Intercuartil (IQR):** Explicaci√≥n de Q1, Q3, IQR y c√°lculo de l√≠mites superior e inferior. Implementaci√≥n en Python.
    * **Z-score:** Explicaci√≥n de su uso para datos que siguen una distribuci√≥n normal. Identificaci√≥n de valores m√°s all√° de 2 o 3 desviaciones est√°ndar.
    * Menci√≥n de otros m√©todos (Isolation Forest, Local Outlier Factor - LOF, para algoritmos de ML).

#### 3. Estrategias de Manejo de Outliers

* **Eliminaci√≥n:** Cu√°ndo es apropiado (errores de medici√≥n, errores de entrada).
* **Transformaci√≥n:**
    * Transformaci√≥n logar√≠tmica (`np.log()`), ra√≠z cuadrada, u otras transformaciones de potencia. Reducen el sesgo y el impacto de valores extremos.
* **Capping (Winsorizaci√≥n):** Limitar los valores de los outliers a un umbral m√°ximo o m√≠nimo (ej. reemplazar valores por encima del percentil 99 con el valor del percentil 99).
* **Manejo con modelos robustos:** Menci√≥n de que algunos modelos son menos sensibles a outliers.

#### 4. Ejercicios Complementarios (M√≥dulo 2.3)

* **Ejercicio 1:** Utiliza boxplots para identificar posibles outliers en la columna 'Precio' y 'Tama√±o de la Tierra' del dataset de Melbourne.
* **Ejercicio 2:** Calcula el IQR para la columna 'Precio' y define los l√≠mites superior e inferior para los outliers. ¬øCu√°ntos outliers identificas usando este m√©todo?
* **Ejercicio 3:** Aplica una transformaci√≥n logar√≠tmica a la columna 'Precio'. Visualiza su distribuci√≥n antes y despu√©s con un histograma. ¬øHa reducido el sesgo y el impacto de los outliers?
* **Ejercicio 4:** Implementa el capping (winsorizaci√≥n) para la columna 'Tama√±o de la Tierra', limitando los valores al percentil 99. Visualiza la distribuci√≥n antes y despu√©s.

### M√≥dulo 2.4: Manejo de Variables Categ√≥ricas

**Contenido:** Los algoritmos de Machine Learning generalmente requieren entradas num√©ricas. Este m√≥dulo te ense√±ar√° c√≥mo convertir variables categ√≥ricas (texto o categor√≠as) en formatos num√©ricos adecuados.

#### 1. Identificaci√≥n y Tipos de Variables Categ√≥ricas

* Revisi√≥n de `df.info()` y `df.select_dtypes(include='object')`.
* Entender la Cardinalidad: `.nunique()`. (Baja, media, alta cardinalidad).
* Tipos de Variables Categ√≥ricas:
    * **Nominales:** Sin orden intr√≠nseco (ej. 'Ciudad', 'Color').
    * **Ordinales:** Con un orden o ranking (ej. 'Peque√±o', 'Mediano', 'Grande').

#### 2. T√©cnicas de Codificaci√≥n (Encoding)

* **One-Hot Encoding:**
    * Explicaci√≥n detallada: Crea nuevas columnas binarias para cada categor√≠a.
    * Uso de `pd.get_dummies()`.
    * Ventajas: No asume orden, adecuado para nominales.
    * Desventajas: Aumenta la dimensionalidad (problema de la maldici√≥n de la dimensionalidad para alta cardinalidad).
* **Label Encoding (o Ordinal Encoding):**
    * Explicaci√≥n: Asigna un n√∫mero entero √∫nico a cada categor√≠a.
    * Uso de `sklearn.preprocessing.LabelEncoder` (para una columna) y `OrdinalEncoder` (para m√∫ltiples columnas, asumiendo un orden).
    * Ventajas: Reduce la dimensionalidad.
    * Desventajas: Introduce un orden artificial si la variable es nominal. Cu√°ndo usarlo.
* **Menci√≥n de otras t√©cnicas avanzadas:**
    * Binary Encoding (para alta cardinalidad, reduce dimensiones m√°s que One-Hot).
    * Target Encoding (codifica categor√≠as bas√°ndose en la media de la variable objetivo, propenso a overfitting).

#### 3. Ejercicios Complementarios (M√≥dulo 2.4)

* **Ejercicio 1:** Identifica todas las columnas categ√≥ricas en el dataset de Melbourne. Para cada una, muestra el n√∫mero de valores √∫nicos y sus 5 valores m√°s frecuentes.
* **Ejercicio 2:** Aplica One-Hot Encoding a la columna 'Tipo de Propiedad'. ¬øCu√°ntas columnas nuevas se crearon?
* **Ejercicio 3:** Asumiendo que la columna 'Regi√≥n' tuviera un orden (ej. 'Norte' < 'Centro' < 'Sur'), aplica Label Encoding. (Nota: esta variable es nominal, el ejercicio es solo para pr√°ctica).
* **Ejercicio 4:** Investiga la columna 'Suburbio'. ¬øSer√≠a apropiado aplicar One-Hot Encoding? ¬øPor qu√© s√≠ o por qu√© no?

### M√≥dulo 2.5: EDA Consolidaci√≥n y Caso Pr√°ctico

**Contenido:** Este es el m√≥dulo donde todo se une. Aplicar√°s todas las t√©cnicas aprendidas en un flujo de trabajo integral sobre el dataset de Melbourne Housing, desde la carga hasta la preparaci√≥n final.

#### 1. Flujo de Trabajo Completo de EDA y Limpieza

* **Carga del Dataset:** Carga el dataset de Melbourne Housing.
* **Inspecci√≥n Inicial y Resumen:** `head()`, `info()`, `describe()`, `shape`, `nunique()`.
* **An√°lisis de Distribuciones y Relaciones:** Revisa las variables clave con histogramas, boxplots, scatter plots, y mapas de calor. **Detecci√≥n de Patrones** a trav√©s de estas visualizaciones.
* **Gesti√≥n de Valores Faltantes:**
    * Cuantifica faltantes.
    * Aplica estrategias de imputaci√≥n adecuadas para cada columna (ej. mediana para 'Precio', moda para 'Habitaciones', o eliminaci√≥n si el porcentaje es muy alto y no es cr√≠tico).
    * Verifica que no queden faltantes.
* **Manejo de Duplicados:**
    * Detecta y elimina filas duplicadas.
* **Detecci√≥n y Tratamiento de Outliers:**
    * Identifica outliers usando boxplots e IQR en columnas clave (ej. 'Precio', 'Tama√±o de la Tierra').
    * Aplica estrategias de manejo (ej. transformaci√≥n logar√≠tmica o capping).
* **Transformaci√≥n de Variables Categ√≥ricas:**
    * Identifica variables categ√≥ricas.
    * Aplica One-Hot Encoding a las variables nominales adecuadas.
    * Aplica Label Encoding si hay variables ordinales.
* **Transformaciones Finales y Escalado de Datos Num√©ricos:**
    * **Escalado de Datos (Normalizaci√≥n/Estandarizaci√≥n):** Explicaci√≥n de por qu√© es importante para muchos algoritmos.
        * `MinMaxScaler` (normaliza a un rango [0,1]).
        * `StandardScaler` (estandardiza a media 0, desviaci√≥n est√°ndar 1).
    * Aplicaci√≥n a las columnas num√©ricas relevantes.
* **Resumen del Dataset Limpio y Preprocesado:**
    * `df.info()` final para ver los tipos de datos y la ausencia de nulos.
    * `df.shape` final para ver las nuevas dimensiones.
    * Discusi√≥n sobre el estado del dataset listo para el modelado.

#### 2. Ejercicios Complementarios (M√≥dulo 2.5)

* **Ejercicio 1:** Aplica un flujo de trabajo completo de EDA y limpieza a un dataset diferente (por ejemplo, el dataset "Boston Housing" o "California Housing" simplificado, o incluso el Titanic, pero aplicando todas las t√©cnicas).
* **Ejercicio 2:** Documenta los pasos de tu flujo de trabajo en un Markdown dentro del notebook, explicando las decisiones que tomaste en cada paso.
* **Ejercicio 3:** Compara el `df.info()` y `df.describe()` del dataset original con el dataset final, preprocesado. ¬øQu√© diferencias observas?
